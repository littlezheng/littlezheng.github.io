<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <link rel="stylesheet" type="text/css" href="/css/style.css" />
    <link rel="shortcut icon" type="image/x-icon" href="/archives/images/永昌空間-logo.ico" />
    <title>永昌空間 | RNN簡述之LSTM與GRU</title>
    <style>
        pre > code{
            color: #006400
        }
    </style>
</head>

<body>

    <article>

        <div class="content">

            <p>
            名詞解釋：<br>
            RNN，Recurrent Neural Network，循環神經網絡。<br>
            LSTM，Long Short Term Memory，長短期記憶。<br>
            GRU，Gated Recurrent Unit，門基循環單元。<br>
            </p>
            
            <div>
                <p>
                LSTM的關鍵是有三個門，分別是，遺忘門（forget gate），輸入門（input gate），
                輸出門（output gate）。這些門使用的是sigmoid函數，取值介於0-1之間。
                通過這些門，有用信息可以隨着時間流到輸出端。
                </p>
                <p>LSTM圖解如下：</p>
                <img src="/archives/2017/04/30/LSTM圖解.jpg" title="LSTM圖解.jpg" alt="LSTM圖解" />
                <p>
                上圖參數的形狀對理解公式很重要，如下所示：<br>
                xt，[input_num, hidden_num] <br>
                ct，[hidden_num, hidden_num] <br>
                ht，[hidden_num, hidden_num] <br>
                ft，it，ot，[hidden_num, hidden_num] <br>
                </p>
            </div>
            
            <div>
                <p>
                GRU的結構比LSTM簡單一些，所以運算量也少一些，但是其效果與LSTM差不多。
                GRU擁有兩個門（r門，z門）。
                </p>
                <p>GRU圖解如下：</p>
                <img src="/archives/2017/04/30/GRU圖解.jpg" title="GRU圖解.jpg" alt="GRU圖解" />
                <p>
                上圖參數的形狀如下：<br>
                xt，[input_num, hidden_num] <br>
                ht，[hidden_num, hidden_num] <br>
                rt，zt，[hidden_num, hidden_num] <br>
                </p>
            </div>
            
            <div>
                <p>LSTM與GRU的輸入輸出問題。</p>
                <p>
                輸入張量，[time_step, batch_size, input_num] <br>
                輸出張量，[time_step, batch_size, hidden_num] <br>
                </p>
                <p>
                其實RNN的信息流動過程很簡單，只需按照時間將其展開即可，如下：
                </p>
                <pre>
                    <code>
for i in range(time_step):
    hiddens[i], state = RNN.predict(inputs[i], state)
                    </code>
                </pre>
                
            </div>
        
        </div>

    </article>

</body>

</html>